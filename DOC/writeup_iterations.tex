%% LyX 2.3.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,notitlepage]{revtex4-1}
\usepackage{tgpagella}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\usepackage{babel}
\usepackage{amsmath}
\usepackage[unicode=true]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{tikz}

\makeatother

\begin{document}
\global\long\def\tht{\vartheta}%
\global\long\def\ph{\varphi}%
\global\long\def\balpha{\boldsymbol{\alpha}}%
\global\long\def\btheta{\boldsymbol{\theta}}%
\global\long\def\bJ{\boldsymbol{J}}%
\global\long\def\bGamma{\boldsymbol{\Gamma}}%
\global\long\def\bOmega{\boldsymbol{\Omega}}%
\global\long\def\d{\text{d}}%
\global\long\def\t#1{\text{#1}}%
\global\long\def\m{\text{m}}%
\global\long\def\v#1{\boldsymbol{#1}}%
\global\long\def\u#1{\underline{#1}}%

\global\long\def\t#1{\mathbf{#1}}%
\global\long\def\bA{\boldsymbol{A}}%
\global\long\def\bB{\boldsymbol{B}}%
\global\long\def\c{\mathrm{c}}%
\global\long\def\difp#1#2{\frac{\partial#1}{\partial#2}}%
\global\long\def\xset{{\bf x}}%
\global\long\def\zset{{\bf z}}%
\global\long\def\qset{{\bf q}}%
\global\long\def\pset{{\bf p}}%

\title{Preconditioned iterations via the Arnoldi method aka incomplete GMRES}
\author{Christopher Albert, Sergei Kasilov}
\date{\today}
\maketitle

\section{Preconditioned iterative schemes}

\subsection{Basics}

Consider a linear system
\begin{equation}
\hat{A}\v x=\v b,\label{eq:Axb}
\end{equation}
where $\v x$ and $\v b$ are vectors of length $N$ and $\hat{A}$
is an invertible linear operator, i.e.
\begin{equation}
\hat{A}(\lambda\v x+\mu\v y)=\lambda\hat{A}\v x+\mu\hat{A}\v y.
\end{equation}
We assume that the matrix representation of as an $N\times N$ is
not given (usually due to the construction of the algorithm and/or
memory requirements for large $N$), but for a given vector $\v x$
there should be an explicit way to obtain $\hat{A}\v x$, e.g. by
a function call. We are thus limited to matrix-free iterative methods
to solve Eq.~(\ref{eq:Axb}). Such methods include direct (Richardson)
iterations, possibly with some preconditioner, the conjugate gradient
method, and the GMRES method.

In particular we are going to consider problems of treating problems
with some ``first guess'' $\v x_{0}$ and a correction $\hat{K}\v x$
depending self-consistently on the final solution, so
\begin{equation}
\v x=\v x_{0}+\hat{K}\v x.\label{eq:xit}
\end{equation}
This means our operator $\hat{A}$ is of the form
\begin{equation}
\hat{A}=(\hat{I}-\hat{K}),
\end{equation}
where $\hat{I}$ is the identity operator, and our equation~(\ref{eq:xit})
in the form of (\ref{eq:Axb}) with $\v b=\v x_{0}$ becomes
\begin{equation}
(\hat{I}-\hat{K})\v x=\v x_{0},\label{eq:ImK}
\end{equation}
so the solution is formally
\begin{equation}
\v x=(\hat{I}-\hat{K})^{-1}\v x_{0}.\label{eq:sol}
\end{equation}


\subsection{Exact solution in the eigenbasis\label{subsec:Exact-solution-in}}

Assume we have have solved the eigenvalue problem for $\hat{K}$ and
can write
\begin{equation}
\hat{K}=\hat{V}\hat{\Lambda}\hat{V}^{-1},
\end{equation}
where
\begin{equation}
\hat{\Lambda}=\left(\begin{array}{cccc}
\lambda_{1}\\
 & \lambda_{2}\\
 &  & \ddots\\
 &  &  & \lambda_{N}
\end{array}\right)=\v{\Lambda}\hat{I}
\end{equation}
is a diagonal matrix containing eigenvalues, 
\begin{equation}
\hat{V}=(\v v_{1},\v v_{2},\dots\v v_{N})
\end{equation}
is a matrix with respective eigenvectors as columns, and $\hat{V}^{-1}$
is its inverse. For\emph{ normal }matrices the eigenvectors form an
orthogonal basis and $\hat{V}^{-1}$ is equal to the transposed complex
conjugate $\hat{V}^{\ast}$ of $\hat{V}$. Multiplying Eq.~(\ref{eq:ImK})
from the left with $\hat{V}^{-1}$ and defining components $\v x^{\prime}$
in the eigenbasis via
\begin{equation}
\v x=\sum_{k}x_{k}^{\prime}\v v_{k}=(\v v_{1},\v v_{2},\dots\v v_{N})\v x^{\prime}=\hat{V}\v x^{\prime},
\end{equation}
yields an equation

\begin{equation}
(\underbrace{\hat{V}^{-1}\hat{I}\hat{V}}_{\hat{I}}-\hat{\Lambda})\v x^{\prime}=\v x_{0}^{\prime},\label{eq:ImK-1}
\end{equation}
where components of $\v x^{\prime}$ and $\v x_{0}^{\prime}$ in the
eigenbasis $(\v v_{1},\v v_{2},\dots\v v_{N})$ are related to $\v x$
and $\v x_{0}$ in the original canonical basis by the inverse basis
transformation
\begin{equation}
\v x^{\prime}=\hat{V}^{-1}\v x.
\end{equation}
For normal matrices with $\hat{V}^{-1}=\hat{V}^{\ast}$ this reduces
to a projection to eigenvectors
\begin{equation}
\v x^{\prime}=\hat{V}^{\ast}\v x\Rightarrow x_{k}^{\prime}=\v v_{k}^{\star}\cdot\v x.
\end{equation}
In any case we can directly write the solution of Eq.~(\ref{eq:ImK-1})
in the eigenbasis,
\begin{equation}
\v x^{\prime}=(\hat{I}-\hat{\Lambda})^{-1}\v x_{0}^{\prime}=\left(\begin{array}{cccc}
\frac{1}{1-\lambda_{1}}\\
 & \frac{1}{1-\lambda_{2}}\\
 &  & \ddots\\
 &  &  & \frac{1}{1-\lambda_{N}}
\end{array}\right)\left(\begin{array}{c}
x_{01}^{\prime}\\
x_{02}^{\prime}\\
\dots\\
x_{0N}^{\prime}
\end{array}\right).
\end{equation}
Finally we transform back to 
\begin{equation}
\v x=\hat{V}\v x^{\prime}.
\end{equation}

Performing this procedure in an incomplete iterative way until convergence,
starting at the largest eigenvalue using a Krylov/Arnoldi scheme is
known as the GMRES method. Since in the end our matrices will be noisy
and our problems possibly nonlinear, we cannot apply standard method
up to convergence, let alone find all eigenvalues. Instead we will
work partly in the eigenbasis to precondition the problem and solve
the remaining (hopefully highly convergent) part of the problem by
direct iterations discussed below.

\subsection{Direct iterations}

A simple and robust (but often slow) way to solve Eq.~(\ref{eq:ImK})
iteratively is the following. If we apply the operator $\hat{K}$
to $\v x$ and it doesn't change anymore, this means that we have
found a converged solution. Indeed we can expand Eq.~(\ref{eq:ImK})
to an operator series analogous to the well-known geometric series
\begin{equation}
\frac{1}{1-z}=1+z+z^{2}+\dots\label{eq:series1}
\end{equation}
as a so-called \emph{Neumann} or \emph{operator series}
\begin{equation}
(\hat{I}-\hat{K})^{-1}=\hat{I}+\hat{K}+\hat{K}^{2}+\dots,
\end{equation}
where $\hat{K}^{n}\equiv\hat{K}\hat{K}\dots$ means applying the operator
$n$ times. The solution~(\ref{eq:sol}) to Eq.~(\ref{eq:ImK})
is thus given as an infinite series
\begin{equation}
\v x=(\hat{I}-\hat{K})^{-1}=\v x_{0}+\hat{K}\v x_{0}+\hat{K}^{2}\v x_{0}+\dots\label{eq:series2}
\end{equation}
Of course we can do this kind of expansion for any linear system Eq.~(\ref{eq:Axb})
with operator $\hat{A}$ in by defining $\hat{K}\equiv\hat{A}+\hat{I}$,
but in contrast to the natural expansion for problems of the form
(\ref{eq:ImK}), adding and substracting $\hat{I}$ in Eq.~(\ref{eq:Axb})
seems like a rather arbitrary step.

\subsubsection{Convergence and analogy to the complex geometric series}

Now the natural question is, if the series (\ref{eq:series2}) converges,
and if yes, how fast. Let us take again the geometric series~(\ref{eq:series1})
as an example. The function $1/(1-z)$ is the inverse of $1-z$ in
the sense that
\begin{align*}
(1-z)\,\v x & =\v x_{0}\Leftrightarrow\v x=\frac{1}{1-z}\v x_{0}
\end{align*}
and has a well-defined value for any complex number $z\neq1$ (the
corresponding operator would be $\hat{K}=z\hat{I}$). We can easily
check convergence of the series by taking the formula for finite polynomials
\begin{equation}
1+z+z^{2}+\dots+z^{n}=\frac{1-z^{n+1}}{1-z}.\label{eq:series1-2}
\end{equation}
Taking a limit $n\rightarrow\infty$ shows lets $z^{n}$ vanish if
$|z|<1$. If we put in a number with $|z|>1$ the series diverges
and cannot reproduce the correct result $1/(1-z)$. The series representation~(\ref{eq:series1})
is thus said to have a convergence radius $r=1$. In the limiting
case $|z|=1$ we don't get any useful information, but note the following:
\begin{enumerate}
\item At a real $z=1$ the series diverges, but so does $\frac{1}{1-z}$.
\item At purely imaginary $z=i$ the series oscillates around zero
\begin{equation}
1+i-1-i+\dots\label{eq:series1-1}
\end{equation}
but doesn't converge.
\end{enumerate}
Now we can ask the question how fast the series converges, i.e. how
many terms we need to compute in order to get a reasonable approximation.
Take for example $z=0.2$. Then we have the exact result
\begin{equation}
f(z)=\frac{1}{1-z}=\frac{1}{1-0.2}=\frac{1}{0.8}=1.25\,.
\end{equation}
The first few truncated series terms are
\begin{align}
f_{[0]} & =1,\\
f_{[1]} & =1+0.2=1.2,\\
f_{[2]} & =1+0.2+0.04=1.24,\\
f_{[3]} & =1+0.2+0.04+0.008=1.248,\\
 & \vdots\nonumber 
\end{align}
We observe that we reach a very good approximation of $1.25$ quite
fast. Take on the other hand $z=0.8$ with
\begin{equation}
f(z)=\frac{1}{1-z}=\frac{1}{1-0.8}=\frac{1}{0.2}=5\,.
\end{equation}
Then we have
\begin{align}
f_{[0]} & =1,\label{eq:08}\\
f_{[1]} & =1+0.8=1.8,\\
f_{[2]} & =1+0.8+0.64=2.44,\\
f_{[3]} & =1+0.8+0.64+0.008=2.952,\label{eq:08end}\\
 & \vdots\nonumber 
\end{align}
Here convergence is much worse and we would need a lot more iterations
to reach values close to $5$. The reason is that $z=0.8$ is much
closer to $1$ than $z=0.2$, and thus produces a large value when
taking $1/(1-z)$. Our available series terms are always $<1$ and
must shrink even further to converge. The exact value of $1/(1-z)$
can become arbitrarily large for $z\rightarrow1$ and arbitrarily
close to $1$ for $z\rightarrow0$. In fact in the former case we
have infinitely slow convergence due to the mentioned reason, whereas
in the latter case we have the exact solution already in the $0$th
iteration.

\subsection{Enforcing convergence of a series by a transformation}

Now the question is how we can extend our radius of convergence beyond
$1$ and at the same time accelerate convergence for $z<1$ close
to $1$. Let's see what happens if we rescale
\begin{equation}
z\equiv aw,
\end{equation}
with a fixed $a$. Then we have partial sums
\begin{equation}
1+aw+a^{2}w^{2}+\dots+a^{n}w^{n}=\frac{1-a^{n+1}w^{n+1}}{1-aw}.
\end{equation}
Unfortunately this doesn't help us at all, since the convergence radius
of the series in $w$ is now only $1/a$, which gives us the exact
same behavior as of the original series. This will turn out to be
the reason why a relaxation factor for direct iterations doesn't help
us in this class or problems.

What works better for accelerating convergence is replacing $z=aw+d$
by a linear transformation with generally complex $a$ and $d$, i.e.
\begin{equation}
\frac{1}{1-z}=\frac{1}{1-aw-d}=\frac{1}{1-d}\frac{1}{1-\frac{aw}{1-d}}.
\end{equation}
More generally, to accelerate convergence of complex number series
one would use conformal mappings (see e.g. \href{http://www.math.wisc.edu/~waleffe/M321/complex.pdf}{http://www.math.wisc.edu/$\sim$waleffe/M321/complex.pdf}),
but for our purpose of treating operators, we limit ourselves to linear
transformations. In that case we obtain a geometric series representation
in $\frac{aw}{1-d}$ as
\begin{equation}
\frac{1}{1-z}=\frac{1}{1-d}\left(1+\frac{aw}{1-d}+\left(\frac{aw}{1-d}\right)^{2}+\dots\right)
\end{equation}
that converges for $\left|\frac{aw}{1-d}\right|<1$, i.e.
\begin{equation}
|z-d|<|1-d|.\label{eq:conve}
\end{equation}
Remember that $d$ is complex, so we can really shift the complex
origin around (try drawing it), rather than just subtracting real
numbers from $z$. Let's take $d=0.6$ and $a=1$. Then for our case
$z=0.8$ we obtain 
\begin{equation}
w=\frac{z-d}{a}=0.2
\end{equation}
 and approximations
\begin{align}
f_{[0]} & =\frac{1}{1-d}=\frac{1}{0.4}=2.5,\\
f_{[1]} & =\frac{1}{0.4}\left(1+\frac{0.8}{0.4}\right)=3.75,\\
f_{[2]} & =\frac{1}{0.4}\left(1+\frac{0.2}{0.4}+\left(\frac{0.2}{0.4}\right)^{2}\right)=4.375,\\
f_{[3]} & =\frac{1}{0.4}\left(1+\frac{0.2}{0.4}+\left(\frac{0.2}{0.4}\right)^{2}+\left(\frac{0.2}{0.4}\right)^{3}\right)=4.6875,\\
 & \vdots\nonumber 
\end{align}
This is not quite as good as the case of $z=0.2$, but already much
better than the direct iterations in~(\ref{eq:08}-\ref{eq:08end}).
Of course if we had taken $d=z$ then $f_{[0]}=f$ would have been
the exact solution with guaranteed convergence everywhere (except
trivial $z=0$) due to $0<|1-z|$ in (\ref{eq:conve}).

\subsection{Extension to operator series}

For the the operator series of $(\hat{I}-\hat{K})^{-1}$ things work
in a similar way as for the scalar case $1/(1-z)\equiv(\hat{I}-z\hat{I})^{-1}$
and can be understood in terms of \emph{eigenvalues}. Say we knew
$\v x_{0}$ in the eigenbasis,
\begin{equation}
\v x_{0}=\sum_{k=1}^{N}x_{0k}^{\prime}\v v_{k}=\hat{V}\v x_{0}^{\prime}.\label{eq:eigdecomp}
\end{equation}
Then
\begin{equation}
\hat{K}\v x_{0}=\hat{V}\hat{\Lambda}\hat{V}^{-1}\v x_{0}=\hat{V}\hat{\Lambda}\v x_{0}^{\prime}=\sum_{k=1}^{N}\lambda_{k}x_{0k}^{\prime}\v v_{k}.
\end{equation}
Applying $\hat{K}$ for $n$ times yields
\begin{equation}
\hat{K}^{n}\v x_{0}=(\hat{V}\hat{\Lambda}\hat{V}^{-1})(\hat{V}\hat{\Lambda}\hat{V}^{-1})\dots(\hat{V}\hat{\Lambda}\hat{V}^{-1})\v x_{0}=\hat{V}\hat{\Lambda}^{n}\v x_{0}^{\prime}=\sum_{k=1}^{N}\lambda_{k}^{\,n}x_{0k}^{\prime}\v v_{k}.
\end{equation}
Thus \emph{all} eigenvalues $\lambda_{k}^{\,n}$ where $x_{0k}^{\prime}\neq0$
i.e. for normal matrices with some components $\v v_{k}^{\star}\cdot\v x_{0}$
of $\v v_{k}$ in the excitation $\v x_{0}$, have to fulfil $\lambda_{k}^{\,n}<1$
for convergence of
\begin{equation}
\hat{I}+\hat{K}+\hat{K}^{2}+\dots
\end{equation}
with direct iterations. We summarize this feature of values for $z$
inside the geometric series for $1/(1-z)$ as well as the spectrum
of eigenvalues $\lambda_{k}$ of $\hat{K}$ for the operator series
in a drawing:
\begin{center}
\scalebox{0.66}{\input{eigs.tpx}}
\par\end{center}

We must get rid of the \emph{ugly} points outside the unit circle
that cause divergence, and would also like to remove \emph{bad} points
close to the inside of the unit circle that cause slow convergence.
With only \emph{good} points left direct iterations will be very fast.

In general we call a method to make an \emph{operator series }converge
by a linear transformation \emph{preconditioned iterations.} More
generally what we have done in the previous section for the seemingly
trivial optimum case $d=z$ is related to the eigenvalues of the operator
where $\hat{K}\v v=\lambda\v v$ for \emph{eigenvectors $\v v$}.
Since for $\hat{K}=z\hat{I}$ all vectors are eigenvectors with $\lambda=z$
the solution becomes trivial in this sense. Now we will extend the
discussed concepts and preconditioning to general linear operators
and their eigenbasis. From section~\ref{subsec:Exact-solution-in}
we already know how to construct an exact solution via the eigenvalue
problem. Now we will only solve a part of the problem in the eigenspace
of the largest eigenvalues for preconditioning. Once we have reached
$\lambda_{k}<1$ (or better $\ll1$) we use direct iterations for
the rest that should rapidly converge if $\lambda_{k}\ll1$. In fact,
for strictly analyzing convergence, one could use the operator norm,
also related to eigenvalues, but here we are satisfied with the more
qualitative explanation and analogy to the complex geometric series.
Krylov/Arnoldi methods are ideally suited for this task, since they
iteratively find the largest eigenvalues (Python and Matlab know this
as ``eigs'' in contrast to the ``eig'' function). In the special
case of Hermitian matrices (they are normal) the Arnoldi method is
known as the Lanczos method.

Let's assume we know the largest $m$ eigenvalues $\lambda_{k}$ of
$\hat{K}$ with eigenvectors $\v v_{k}$, $k=1\dots m$. Then we introduce
the $m\times N$ incomplete eigenvector matrix
\begin{equation}
\hat{V}_{m}=(\v v_{1},\v v_{2},\dots,\v v_{m}).
\end{equation}
Now for a \emph{normal }operator $\hat{K}$ it would be easy to just
project down to the subspace spanned by $\v v_{1\dots m}$ via an
inner product $\v v_{k}^{\star}\cdot\v x$. However, for a general
(not necessarily normal) operator $\hat{K}$ the eigenvectors are
not orthogonal, i.e. $\v v_{i}^{\star}\cdot\v v_{j}\neq|\v v_{j}|^{2}\delta_{ij}$.
This case is similar to the extension of tensor calculus to a general
non-orthogonal basis in co- and contravariant formalism. The case
is handled by the metric tensor components in geometry, that has components
\begin{equation}
g_{ij}=\v e_{i}\cdot\v e_{j}
\end{equation}
and becomes $\delta_{ij}$ in an orthonormal system. We can convert
between co- and contravariant components by $g_{ij}$ and its inverse
matrix $g^{ij}$ for the reciprocal metric. Components of vectors
can be found by projecting to the dual basis, i.e.
\begin{align}
\v a & =\sum_{k}a^{k}e_{k}=\sum_{k}(\v e^{k}\cdot\v a)\v e_{k}\nonumber \\
 & =\sum_{i,k}a_{i}g^{ik}\v e_{k}=\sum_{i,k}(\v e_{i}\cdot\v a)g^{ik}\v e_{k}.
\end{align}
We see that in order to work fully in the usual basis $\v e_{k}$
and avoid the dual basis $\v e^{k}$ we require the reciprocal metric
$g^{ik}$. Also the ``bra-ket'' notation of quantum mechanics works
the same way.

Analogously in our complex linear operator space to be able to project
to the partial eigenbasis in $\hat{V}_{m}$ we define a matrix $\hat{G}$
with entries
\begin{equation}
G_{ij}=\v v_{i}^{\star}\cdot\v v_{j}
\end{equation}
corresponding to the projections between possibly non-orthogonal eigenvectors
and invert it to $\hat{G}^{-1}$ with components $G^{ik}$. Remember
that
\begin{equation}
\v x^{\prime}=\hat{V}^{-1}\v x
\end{equation}
in the full eigenspace. Now we cannot get $\hat{V}^{-1}$ of the full
eigenspace here but at least project down to the partial eigenspace
spanned by $\hat{V}_{m}$. There we have
\begin{equation}
\v x=\sum_{k=1}^{m}x_{k}^{\prime}\v v_{k}+\bar{\v x}_{0}\label{eq:eigbasis}
\end{equation}
where $\bar{\v x}_{0}$ is the part that cannot be written in terms
of columns of $\hat{V}_{m}$, i.e. is orthogonal with
\begin{equation}
\v v_{k}^{\star}\cdot\bar{\v x}_{0}=0\quad\text{for }k=1\dots m.
\end{equation}
If we take an inner product Eq.~(\ref{eq:eigbasis}) with $\v v_{i}^{\star}$
with $k<m$ we obtain
\begin{equation}
\v v_{i}^{\star}\cdot\v x=\sum_{k=1}^{m}x_{k}^{\prime}\v v_{i}^{\star}\cdot\v v_{k}=\sum_{k=1}^{m}G_{ik}x_{k}^{\prime},\label{eq:eigbasis-1}
\end{equation}
or, in operator notation
\begin{equation}
\hat{V}_{m}^{\star}\v x=\hat{G}\v x_{m}^{\prime},
\end{equation}
where $\v x_{m}^{\prime}$ contains the first $m$ entries of $\v x^{\prime}$.
To obtain $\v x_{m}^{\prime}$ we transform this to
\begin{equation}
\v x_{m}^{\prime}=\hat{G}^{-1}\hat{V}_{m}^{\star}\v x.
\end{equation}
or, in components
\begin{equation}
x_{i}^{\prime}=\sum_{k=1}^{m}G^{ik}\v v_{k}^{\star}\cdot\v x\quad\text{for }i=1\dots m.\label{eq:eigbasis-1-1}
\end{equation}
Apply $\hat{G}^{-1}\hat{V}_{m}^{\star}$ to the original equation
\begin{equation}
(\hat{I}-\hat{K})\v x=\v x_{0},
\end{equation}
resulting in the left-hand side
\begin{equation}
\hat{G}^{-1}\hat{V}_{m}^{\star}(\hat{I}-\hat{K})\v x=\hat{G}^{-1}((\hat{I}-\hat{K})^{-1}\hat{V}_{m}^{-1})^{-1}\v x
\end{equation}


\section{TODOs for the one who continues kinetic equilibria:}

\subsection{Perturbed problem for noisy matrices}

Check how sensitive the method is to random noise in the operator
$\hat{A}$ and possibly find some general rules of applicability.

\subsection{Extension to nonlinear operators}

Extend the method to nonlinear operators.
\end{document}
